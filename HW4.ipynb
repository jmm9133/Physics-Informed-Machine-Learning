{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "@author: josephmurray\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "class NeuralNetwork:\n",
    "    def sigmoid(Z):\n",
    "        #print(\"Z_input=\",Z)\n",
    "        Z = 1/(1+np.exp(-Z))\n",
    "        #print(\"Z =\",Z)\n",
    "        return Z\n",
    "\n",
    "    def relu(Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def sigmoid_backward(dA, Z):\n",
    "        sig = NeuralNetwork.sigmoid(Z)\n",
    "        return dA * sig * (1 - sig)\n",
    "\n",
    "    def relu_backward(dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        #print(dZ.shape)\n",
    "        #print(Z.shape)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    def target_function(x, y):\n",
    "        return np.sin(2 * np.pi * x * y) + 2 * x * y ** 2\n",
    "\n",
    "    def set_layers(nn):\n",
    "        np.random.seed(50)\n",
    "        numb_layers = len(nn)\n",
    "        params_values = {}\n",
    "        for idx,layer in enumerate(nn):\n",
    "            layer_idx = idx +1\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            params_values['W' + str(layer_idx)] = np.random.randn(layer_input_size,\n",
    "                layer_output_size) * 0.1\n",
    "            params_values['b' + str(layer_idx)] = np.random.randn(layer_output_size,\n",
    "                1) * 0.1\n",
    "\n",
    "        return params_values\n",
    "\n",
    "    def forward_pass(x,nn,params_values):\n",
    "        num_layers = len(nn)\n",
    "        #print(\"Input:\",x)\n",
    "        x = np.array(x).reshape(-1, 1)\n",
    "        #x = np.transpose(x)\n",
    "        \n",
    "        for idx,num_layers in enumerate(nn):\n",
    "            layer_idx = idx+1\n",
    "            val_layer = np.dot(x,params_values['W'+ str(layer_idx)])+params_values['b' + str(layer_idx)]\n",
    "            activation = NeuralNetwork.relu(val_layer)\n",
    "            x = activation\n",
    "        output = x\n",
    "        #print(\"Output:\",output)\n",
    "        return output\n",
    "    def loss1D(output, true):\n",
    "        mse_loss = np.mean((true - output) ** 2)\n",
    "        return mse_loss\n",
    "\n",
    "    def accuracy(output, true):\n",
    "        correct_predictions = np.sum(output == true)  # Count matching elements\n",
    "        accuracy_percentage = (correct_predictions / len(true)) * 100  # Convert to percentage\n",
    "        return accuracy_percentage\n",
    "\n",
    "# Function Definitions\n",
    "def target_function(x, y):\n",
    "    return np.sin(2 * np.pi * x * y) + 2 * x * y ** 2\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    #print(\"Input Layer Size:\", A_prev.shape)\n",
    "    #print(\"Weight Matrix size:\",W_curr.shape)\n",
    "    #print(\"Bias Matrix size:\",b_curr.shape)\n",
    "    Z_curr = np.dot(A_prev,W_curr)\n",
    "    #print(\"Weight output:\",Z_curr)\n",
    "    Z_curr =Z_curr + np.transpose(b_curr)\n",
    "    #print(\"Output Size:\",Z_curr.shape)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        activation_func = NeuralNetwork.relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = NeuralNetwork.sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    #print(\"Output:\",activation_func(Z_curr))\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    #print(\"A_initial\",X)\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        #print(f\"Layer {idx+1}: Input Dim: {layer['input_dim']}, Output Dim: {layer['output_dim']}, Activation: {layer['activation']}\")\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        #print(\"W_size\", W_curr.size)\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        \n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "    #print(memory)\n",
    "    #print(\"Final Output= \",A_curr)\n",
    "    return A_curr, memory\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape\n",
    "    if np.isscalar(Y):\n",
    "        dA_prev = (Y_hat-Y)\n",
    "    else:\n",
    "        dA_prev = (Y_hat-Y)/Y.shape[0]\n",
    "   \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev +1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    #print(A_prev.shape[1])\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = NeuralNetwork.relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = NeuralNetwork.sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(A_prev.T,dZ_curr)\n",
    "    db_curr = np.sum(dZ_curr, axis=0)\n",
    "    db_curr = db_curr[:, np.newaxis]\n",
    "    dA_prev = np.dot(dZ_curr,W_curr.T)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def forward_auto_diff(Y_hat, Y, params_values, nn_architecture, X):\n",
    "\n",
    "    memory = {}\n",
    "    A_curr = X  \n",
    "    dA_curr = np.ones_like(X)\n",
    "    memory[\"A0\"] = X\n",
    "    memory[\"dA0\"] = dA_curr\n",
    "\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx_curr = layer_idx + 1\n",
    "\n",
    "        # Get parameters for the current layer\n",
    "        W = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        activation = layer[\"activation\"]\n",
    "\n",
    "        # Forward pass (primary variables)\n",
    "        Z_curr = np.dot(A_curr, W) + b.T\n",
    "        dZ_curr = np.dot(dA_curr, W)  # Derivative wrt input to this layer\n",
    "\n",
    "        # Apply activation function and compute its derivative\n",
    "        if activation == \"relu\":\n",
    "            A_curr = NeuralNetwork.relu(Z_curr)\n",
    "            dA_curr = dZ_curr * np.where(Z_curr > 0, 1, 0)  # Chain rule\n",
    "        elif activation == \"sigmoid\":\n",
    "            A_curr = NeuralNetwork.sigmoid(Z_curr)\n",
    "            dA_curr = dZ_curr * (A_curr * (1 - A_curr))  # Chain rule\n",
    "        else:\n",
    "            raise Exception(\"Non-supported activation function\")\n",
    "\n",
    "        # Store values and their derivatives\n",
    "        memory[\"A\" + str(layer_idx_curr)] = A_curr\n",
    "        memory[\"Z\" + str(layer_idx_curr)] = Z_curr\n",
    "        memory[\"dA\" + str(layer_idx_curr)] = dA_curr\n",
    "\n",
    "    return A_curr, memory  # A_curr is Y_hat\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        # Update weights and biases using the gradients corresponding to the random index\n",
    "        params_values[\"W\" + str(layer_idx+1)] -= learning_rate * grads_values[\"dW\" + str(layer_idx+1)].T      \n",
    "        params_values[\"b\" + str(layer_idx+1)] -= learning_rate * grads_values[\"db\" + str(layer_idx+1)]\n",
    "\n",
    "    return params_values\n",
    "def update_forward(params_values, memory, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx_curr = layer_idx + 1\n",
    "\n",
    "        # Retrieve gradients from memory\n",
    "        A_prev = memory[\"A\" + str(layer_idx)]\n",
    "        dA_curr = memory[\"dA\" + str(layer_idx_curr)]\n",
    "\n",
    "        # Compute gradients for weights and biases\n",
    "        dW_curr = np.dot(A_prev.T, dA_curr)  # Gradient wrt weights\n",
    "        db_curr = np.sum(dA_curr, axis=0)  # Gradient wrt biases\n",
    "        db_curr = db_curr[:, np.newaxis]\n",
    "        # Update weights and biases using gradient descent\n",
    "        print(dW_curr)\n",
    "        print(db_curr)\n",
    "        params_values[\"W\" + str(layer_idx_curr)] -= learning_rate * dW_curr\n",
    "        params_values[\"b\" + str(layer_idx_curr)] -= learning_rate * db_curr\n",
    "\n",
    "    return params_values\n",
    "def train(target_function, nn_architecture, epochs, learning_rate, verbose=True,trainopt='backwa'):\n",
    "    params_values = NeuralNetwork.set_layers(nn_architecture)\n",
    "    cost_history = []\n",
    "    best_cost = float('inf')\n",
    "    best_params = None\n",
    "    patience = 5000\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    initial_lr = learning_rate\n",
    "    \n",
    "    # Generate the entire batch of training data\n",
    "    X = np.random.uniform(0,1,1000)\n",
    "    y = np.random.uniform(0,1,1000)\n",
    "    X = np.column_stack((X,y))# Replace 1000 with the desired dataset size\n",
    "    Y = np.array([target_function(x, y) for x, y in X]).reshape(-1, 1)\n",
    "    print(params_values)\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass for the entire dataset\n",
    "        Y_hat, cache = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # Calculate loss for the batch\n",
    "        cost = NeuralNetwork.loss1D(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        \n",
    "        # Backward pass for the entire dataset\n",
    "        if trainopt == 'backward':\n",
    "            grads_values = full_backward_propagation(Y_hat, Y, cache, params_values, nn_architecture)\n",
    "            params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        else:\n",
    "            _, grads_values = forward_auto_diff(Y_hat, Y, params_values, nn_architecture,X)\n",
    "            params_values = update_forward(params_values, grads_values, nn_architecture, learning_rate)\n",
    "            print(params_values)\n",
    "        \n",
    "        # Update parameters with current learning rate\n",
    "        \n",
    "        \n",
    "        # Early stopping check\n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "            best_params = {key: value.copy() for key, value in params_values.items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch % 1 == 0 or epoch == epochs - 1):\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {cost:.6f}, Learning Rate: {learning_rate:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Return the best parameters found during training\n",
    "    return best_params, cost_history\n",
    "def compare_gradient_computations(nn_architecture, params_values, num_tests=100):\n",
    "    \n",
    "    metrics = {\n",
    "        'forward_times': [],\n",
    "        'backward_times': [],\n",
    "        'gradient_differences': [],\n",
    "        'forward_gradients': [],\n",
    "        'backward_gradients': []\n",
    "    }\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        # Generate random test point\n",
    "        x = np.random.rand()\n",
    "        y = np.random.rand()\n",
    "        X = np.array([x, y]).reshape(1, 2)\n",
    "        Y = target_function(x, y)\n",
    "        \n",
    "        # Time forward autodiff\n",
    "        start_time = time.time()\n",
    "        Y_hat, memory = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        _, forward_gradients = forward_auto_diff(Y_hat,Y,params_values, nn_architecture,X)\n",
    "        forward_time = time.time() - start_time\n",
    "        metrics['forward_times'].append(forward_time)\n",
    "        \n",
    "        # Time backward propagation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture)\n",
    "        backward_time = time.time() - start_time\n",
    "        metrics['backward_times'].append(backward_time)\n",
    "        \n",
    "        # Compute gradient norms\n",
    "        forward_norm = np.sum([np.linalg.norm(forward_gradients[\"dA\" + str(layer_idx + 1)]) \n",
    "                              for layer_idx in range(len(nn_architecture))])\n",
    "        backward_norm = np.sum([np.linalg.norm(grads_values[\"dW\" + str(layer_idx + 1)]) \n",
    "                              for layer_idx in range(len(nn_architecture))])\n",
    "        \n",
    "        # Store gradients and their difference\n",
    "        metrics['forward_gradients'].append(forward_norm)\n",
    "        metrics['backward_gradients'].append(backward_norm)\n",
    "        metrics['gradient_differences'].append(abs(forward_norm - backward_norm))\n",
    "    \n",
    "    # Compute summary statistics\n",
    "    summary = {\n",
    "        'avg_forward_time': np.mean(metrics['forward_times']),\n",
    "        'avg_backward_time': np.mean(metrics['backward_times']),\n",
    "        'std_forward_time': np.std(metrics['forward_times']),\n",
    "        'std_backward_time': np.std(metrics['backward_times']),\n",
    "        'avg_gradient_diff': np.mean(metrics['gradient_differences']),\n",
    "        'max_gradient_diff': np.max(metrics['gradient_differences']),\n",
    "        'min_gradient_diff': np.min(metrics['gradient_differences'])\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGradient Computation Comparison Results:\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(f\"Average Forward Time: {summary['avg_forward_time']:.6f} ± {summary['std_forward_time']:.6f} seconds\")\n",
    "    print(f\"Average Backward Time: {summary['avg_backward_time']:.6f} ± {summary['std_backward_time']:.6f} seconds\")\n",
    "    print(f\"\\nGradient Difference Statistics:\")\n",
    "    print(f\"Average Difference: {summary['avg_gradient_diff']:.6f}\")\n",
    "    print(f\"Maximum Difference: {summary['max_gradient_diff']:.6f}\")\n",
    "    print(f\"Minimum Difference: {summary['min_gradient_diff']:.6f}\")\n",
    "    \n",
    "    # Calculate speed comparison\n",
    "    speedup = summary['avg_backward_time'] / summary['avg_forward_time']\n",
    "    print(f\"\\nSpeed Comparison:\")\n",
    "    print(f\"Forward method is {speedup:.2f}x {'faster' if speedup > 1 else 'slower'} than backward method\")\n",
    "    \n",
    "    return metrics, summary\n",
    "def test_neural_network(nn_architecture, trained_params, num_test_points=1000, plot_results=True):\n",
    "    \"\"\"\n",
    "    Test the trained neural network and visualize its performance.\n",
    "    \n",
    "    Args:\n",
    "        nn_architecture: Neural network architecture configuration\n",
    "        trained_params: Trained network parameters\n",
    "        num_test_points: Number of test points to evaluate\n",
    "        plot_results: Whether to generate visualization plots\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing test metrics and results\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    # Generate test points\n",
    "    x_test = np.random.rand(num_test_points)\n",
    "    y_test = np.random.rand(num_test_points)\n",
    "    \n",
    "    # Arrays to store results\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    errors = []\n",
    "    \n",
    "    # Test the network\n",
    "    for i in range(num_test_points):\n",
    "        # Get true value\n",
    "        true_value = target_function(x_test[i], y_test[i])\n",
    "        true_values.append(true_value)\n",
    "        \n",
    "        # Get prediction\n",
    "        X = np.array([x_test[i], y_test[i]]).reshape(1, 2)\n",
    "        prediction, _ = full_forward_propagation(X, trained_params, nn_architecture)\n",
    "        predictions.append(prediction[0][0])\n",
    "        \n",
    "        # Calculate error\n",
    "        error = abs(prediction[0][0] - true_value)\n",
    "        errors.append(error)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mean_error = np.mean(errors)\n",
    "    max_error = np.max(errors)\n",
    "    min_error = np.min(errors)\n",
    "    std_error = np.std(errors)\n",
    "    mse = np.mean(np.square(errors))\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nNeural Network Test Results:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Number of test points: {num_test_points}\")\n",
    "    print(f\"Mean Absolute Error: {mean_error:.6f}\")\n",
    "    print(f\"Max Error: {max_error:.6f}\")\n",
    "    print(f\"Min Error: {min_error:.6f}\")\n",
    "    print(f\"Standard Deviation of Error: {std_error:.6f}\")\n",
    "    print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "    \n",
    "    if plot_results:\n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: True vs Predicted Surface\n",
    "        ax1 = fig.add_subplot(131, projection='3d')\n",
    "        ax1.scatter(x_test, y_test, true_values, c='blue', label='True Values', alpha=0.5)\n",
    "        ax1.scatter(x_test, y_test, predictions, c='red', label='Predictions', alpha=0.5)\n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "        ax1.set_zlabel('Z')\n",
    "        ax1.set_title('True vs Predicted Values')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 3: Predicted vs True Values Scatter\n",
    "        ax3 = fig.add_subplot(132)\n",
    "        ax3.scatter(true_values, predictions, alpha=0.5)\n",
    "        ax3.plot([min(true_values), max(true_values)], \n",
    "                 [min(true_values), max(true_values)], \n",
    "                 'r--', label='Perfect Prediction')\n",
    "        ax3.set_xlabel('True Values')\n",
    "        ax3.set_ylabel('Predicted Values')\n",
    "        ax3.set_title('Predicted vs True Values')\n",
    "        ax3.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'predictions': predictions,\n",
    "        'true_values': true_values,\n",
    "        'errors': errors,\n",
    "        'metrics': {\n",
    "            'mean_error': mean_error,\n",
    "            'max_error': max_error,\n",
    "            'min_error': min_error,\n",
    "            'std_error': std_error,\n",
    "            'mse': mse\n",
    "        },\n",
    "        'test_points': {\n",
    "            'x': x_test,\n",
    "            'y': y_test\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "def visualize_training(cost_history):\n",
    "    \"\"\"\n",
    "    Visualize the training progress.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cost_history)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "# Part A\n",
    "params_values= NeuralNetwork.set_layers(nn_architecture)\n",
    "x = np.random.rand()\n",
    "y = np.random.rand()\n",
    "x_true = target_function(x,y)\n",
    "#jacobian_forward, grads_reverse = compute_gradients(x_true, nn_architecture, params_values)\n",
    "\n",
    "\n",
    "X_input = [x,y]\n",
    "#output = NeuralNetwork.forward_pass(X_input,nn_architecture,params_values)\n",
    "#print(\"Inputs:\", X_input)\n",
    "#print(\"Forward pass outputs:\", output)\n",
    "#print(\"True output:\",x_true)\n",
    "metrics,summary = compare_gradient_computations(nn_architecture,params_values,100)\n",
    "#test_forward_backward_consistency(nn_architecture,params_values,x,y)\n",
    "NN_trained_forward,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'b')\n",
    "NN_trained,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'b')\n",
    "#visualize_training(costhistory)\n",
    "#results = test_neural_network(nn_architecture,NN_trained,10000,True)\n",
    "\n",
    "## Part B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture= [\n",
    "    {\"input_dim\": 2, \"output_dim\":10, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 10, \"output_dim\":10, \"activation\":\"relu\"},\n",
    "    {\"input_dim\": 10, \"output_dim\":10, \"activation\":\"relu\"},\n",
    "    {\"input_dim\": 10, \"output_dim\":10, \"activation\":\"relu\"},\n",
    "    {\"input_dim\": 10, \"output_dim\":1, \"activation\":\"relu\"},\n",
    "]\n",
    "## Part A\n",
    "params_values= NeuralNetwork.set_layers(nn_architecture)\n",
    "x = np.random.rand()\n",
    "y = np.random.rand()\n",
    "x_true = target_function(x,y)\n",
    "X_input = [x,y]\n",
    "output = NeuralNetwork.forward_pass(X_input,nn_architecture,params_values)\n",
    "print(\"Inputs:\", X_input)\n",
    "print(\"Forward pass outputs:\", output)\n",
    "print(\"True output:\",x_true)\n",
    "#metrics,summary = compare_gradient_computations(nn_architecture,params_values,100)\n",
    "#test_forward_backward_consistency(nn_architecture,params_values,x,y)\n",
    "#NN_trained_forward,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'b')\n",
    "#NN_trained,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'b')\n",
    "#visualize_training(costhistory)\n",
    "#results = test_neural_network(nn_architecture,NN_trained,10000,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B\n",
    "metrics,summary = compare_gradient_computations(nn_architecture,params_values,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C\n",
    "\n",
    "NN_trained_forward,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_trained,costhistory = train(target_function,nn_architecture,epochs=1000000, learning_rate = .01, verbose=True,trainopt = 'backward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmmlbasic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
